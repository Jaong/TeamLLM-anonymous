# Future Scenarios, Responses and human Scores Dataset (English)

This folder contains two files:

- `future_scenarios_en.json`: One future scenario (FS10), developed for our proposed **CGPST (Contextually-Grounded and Procedurally-Structured Tasks)**. The scenario is an open-ended, narrative-based future scenario situated approximately 15–20 years into the future (see Section 4.1 of the paper).

- `data_A.json`: The complete response generated by the `kimi-k2-0711-preview` under the `TeamLLM` condition for the same future scenario (FS10), with the response ID `A05_FS10`, along with the corresponding evaluation data provided by two expert raters (`H01` and `H02`).

All of the above materials are presented in Appendix L of the paper. The formats of both data files are described in the Format section.

---

## Format of Dataset

- `future_scenarios_en.json`


    ```
    {
        "FSx": {  
            "theme": "Theme of FSx",
            "text": "Future scenario of FSx"
        },
        
        "FSy": {
            "theme": "Theme of FSy",
            "text": "Future scenario of FSy"
        }
    }
    ```

- `data_A.json`: This file is in **list** format, where each element is itself a **list** representing one model response together with the corresponding evaluations from two human experts.
Each element (in list form) contains the following items **in order**:

  1. **Response_ID**  
   A string representing the response identifier.

  2. **Human_ID**  
   A list containing two string elements, representing the IDs of the two human experts who evaluated the response.

  3. **Future_Scenario_ID**  
   A string representing the future scenario identifier.

  4. **Scores from Human Expert 1**  
   A dictionary containing a total of **7 key–value pairs**, including `"Step-1"` to `"Step-6"` and `"Total Score"`.  
   All values are stored in **list** format. The detailed structure is as follows:

   - **(1) Step-1, Step-3**  
     Each step is represented as a list containing multiple elements. Each element is a dictionary corresponding to one response item (a challenge in Step-1 or a solution in Step-3). Each dictionary first contains:
     - `"Content"`: the model-generated response content.

     The remaining fields depend on whether the response is valid (see Appendix J of the paper):

     - **(i) Valid response**  
       The dictionary further includes:
       - `"Category"`: the category selected by the human rater.
       - `"Originality"`: the originality score assigned by the human rater.
       - `"Elaboration"`: the elaboration score assigned by the human rater.

     - **(ii) Invalid response**  
       The dictionary instead includes:
       - `"Invalid"`: the reason selected by the human rater for marking the response as invalid (e.g., *Solution8* in *Step-3* in the example below).

     For all response items, the final key in each dictionary is:
     - `"Score Summary"`: a dictionary summarizing the total scores across all dimensions for that step.

   - **(2) Step-4**  
   Each step is represented as a list containing multiple elements. Each element is a dictionary corresponding to one criterion. Each dictionary contains:
     - `"Content"`: the model-generated response content.
     - `"Correctly Written"`: the dimension score assigned by the human rater.
     - `"Relevance"`: the dimension score assigned by the human rater.

   - **(3) Step-2, Step-5, Step-6**  
     Each of these steps contains only **one element** in the list. This element is a dictionary in which:
     - the first key–value pair is `"Content"`: the model-generated response content  
     - the remaining key–value pairs correspond to all scoring dimensions for that step and their associated scores.

   - **(4) Total Score**  
     In addition to the `"Step-1"` to `"Step-6"` entries, the final key–value pair in the dictionary is:
     - `"Total Score"`: the overall score assigned by Human Expert 1 for this response.

  5. **Scores from Human Expert 2**  
   The structure is identical to that of Human Expert 1.

  **Summary:** Overall, the file is a **list of lists**, where each inner list corresponds to one response and the evaluations provided by two human experts, structured as: ["Response_ID", ["Human1_ID", "Human2_ID"], "Future_Scenario_ID", {Human1_Score}, {Human2_Score}]


    ```
    [
        [
            "A05_FS10", ["H01", "H02"], "FS10",
            {
                "Step-1": [
                    {
                        "Content": "Challenge1",
                        "Category": "Environment",
                        "Originality": 0,
                        "Elaboration": 1
                    },
                    ...
                    {
                        "Content": "Challenge8",
                        "Category": "Technology",
                        "Originality": 0,
                        "Elaboration": 1
                    },
                    {
                        "Score Summary": {
                            "Fluency": 8,
                            "Flexibility": 5,
                            "Elaboration": 8,
                            "Originality": 3,
                            "Overall": 24
                        }
                    }
                ],
                "Step-2": [
                    {
                        "Content": "Responses",
                        "Condition Phrase": 2,
                        "Stem & KVP": 3,
                        "Purpose": 1,
                        "FS Parameters": 2,
                        "Focus": 6,
                        "Adequacy": 8,
                        "Overall": 22
                    }
                ],
                "Step-3": [
                    {
                        "Content": "Solution1",
                        "Category": "Technology",
                        "Originality": 0,
                        "Elaboration": 2
                    },
                    ...
                    {
                        "Content": "Solution8",
                        "Invalid": "Duplicate — The solution is too similar to another YES solution"
                    },
                    {
                        "Score Summary": {
                            "Fluency": 7,
                            "Flexibility": 2,
                            "Elaboration": 14,
                            "Originality": 7,
                            "Overall": 30
                        }
                    }
                ],
                "Step-4": [
                    {
                        "Content": "Criteria1",
                        "Correctly Written": 1,
                        "Relevance": 2
                    },
                    ...
                    {
                        "Content": "Criteria5",
                        "Correctly Written": 1,
                        "Relevance": 2
                    },
                    {
                        "Score Summary": {
                            "Correctly Written": 5,
                            "Relevance": 6,
                            "Overall": 11
                        }
                    }
                ],
                "Step-5": [
                    {
                        "Content": "Response",
                        "Correctly Used": 4,
                        "Overall": 4
                    }
                ],
                "Step-6": [
                    {
                        "Content": "Response",
                        "Relevance": 3,
                        "Effectiveness": 3,
                        "Criteria": 4,
                        "Impact": 4,
                        "Humaness": 3,
                        "Development": 8,
                        "Overall": 25
                    }
                ],
                "Total Score": 116
            },
            {
                "Step-1": [
                    {
                        "Content": "Challenge1",
                        "Category": "Environment",
                        "Originality": 0,
                        "Elaboration": 1
                    },
                    ...
                    {
                        "Content": "Challenge2",
                        "Category": "Communication",
                        "Originality": 1,
                        "Elaboration": 1
                    },
                    {
                        "Score Summary": {
                            "Fluency": 8,
                            "Flexibility": 5,
                            "Elaboration": 8,
                            "Originality": 7,
                            "Overall": 28
                        }
                    }
                ],
                "Step-2": [
                    {
                        "Content": "Response",
                        "Condition Phrase": 2,
                        "Stem & KVP": 3,
                        "Purpose": 1,
                        "FS Parameters": 2,
                        "Focus": 6,
                        "Adequacy": 8,
                        "Overall": 22
                    }
                ],
                "Step-3": [
                    {
                        "Content": "Solution1",
                        "Category": "Technology",
                        "Originality": 1,
                        "Elaboration": 2
                    },
                    ...
                    {
                        "Content": "solution8",
                        "Invalid": "Duplicate — The solution is too similar to another YES solution"
                    },
                    {
                        "Score Summary": {
                            "Fluency": 7,
                            "Flexibility": 2,
                            "Elaboration": 14,
                            "Originality": 9,
                            "Overall": 32
                        }
                    }
                ],
                "Step-4": [
                    {
                        "Content": "Criteria1",
                        "Correctly Written": 1,
                        "Relevance": 2
                    },
                    ...
                    {
                        "Content": "Criteria5",
                        "Correctly Written": 1,
                        "Relevance": 2
                    },
                    {
                        "Score Summary": {
                            "Correctly Written": 5,
                            "Relevance": 8,
                            "Overall": 13
                        }
                    }
                ],
                "Step-5": [
                    {
                        "Content": "Response",
                        "Correctly Used": 4,
                        "Overall": 4
                    }
                ],
                "Step-6": [
                    {
                        "Content": "Response",
                        "Relevance": 3,
                        "Effectiveness": 3,
                        "Criteria": 3,
                        "Impact": 3,
                        "Humaness": 2,
                        "Development": 4,
                        "Overall": 18
                    }
                ],
                "Total Score": 117
            }
        ],
        ...
    ]
    ```

## NOTE
- We commit to releasing all ten future scenarios and all 200 responses and scores upon paper acceptance. At this stage, only one scenario (`FS10`) and one response (`A05_FS10`) are publicly released to facilitate readers’ understanding of our task design and methodology. This released scenario and scores is identical to the example presented in Appendix H of the paper.